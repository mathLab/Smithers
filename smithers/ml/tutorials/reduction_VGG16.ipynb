{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction of VGG16\n",
    "In this tutorial we will present how to create a reduced version of VGG16 using the techniques described in the article ''A Dimensionality Reduction Approach for Convolutional Neural Networks'', Meneghetti L., Demo N., Rozza G., https://arxiv.org/abs/2110.09163 (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We start by importing all the necessary libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/scratch/lmeneghe/Smithers/')\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "from smithers.ml.models.vgg import VGG\n",
    "from smithers.ml.models.utils_rednet import get_seq_model, Total_param, Total_flops, compute_loss, train_kd\n",
    "from smithers.ml.models.netadapter import NetAdapter\n",
    "\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the proper device\n",
    "The following lines will detect if a gpu is available in the system running this tutorial. If that is the case, all the objects of the following tutorial will be allocated in the gpu, thus speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda has been detected as the device which the script will be run on.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"{device} has been detected as the device which the script will be run on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the dataset\n",
    "### CIFAR10 Dataset\n",
    "We use the CIFAR10 dataset (already implemented in PyTorch) to test our technique. It is a computer-vision dataset used for object recognition. It consists of 60000 32 Ã— 32 colour images divided in 10 non-overlapping classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
    "\n",
    "See https://www.cs.toronto.edu/~kriz/cifar.html for more details on this dataset and on how to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 \n",
    "data_path = '../cifar/' \n",
    "# transform functions: take in input a PIL image and apply this\n",
    "# transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "train_labels = torch.tensor(train_loader.dataset.targets)\n",
    "targets = list(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset\n",
    "If we want to use a custom dataset, we need firstly to construct it, following for example the tutorial on the construction of a custom dataset for the problem of Image Recognition (***customdata_imagerec***). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the model\n",
    "First of all we need to load the model we want to use (in this case VGG16) starting from a checkpoint file, i.e. a file containing the status of the model after a training process with a chosen dataset. Here we will use the CIFAR10 dataset, but everythong can be also generalized for a custom dataset or another benchmark dataset.\n",
    "\n",
    "It is important to highlight that the models of VGG-nets implemented in PyTorch (https://pytorch.org/hub/pytorch_vision_vgg/), e.g. \n",
    "```\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True),\n",
    "```\n",
    "\n",
    "are models pre-trained on the ImageNet dataset, that consists of images of dimensions 224x224. Therefore, in order to use datasets like the CIFAR10, composed of images 32x32, we need to change the architecture of VGG-nets, as was done in the file ***smithers/ml/vgg.py***.\n",
    "\n",
    "In order to obtain a checkpoint file, the tutorial ***training_VGG16*** can be followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained = insert here the proper path for your device\n",
    "pretrained = 'check_vgg.pth'\n",
    "VGGnet = torch.load(pretrained)\n",
    "seq_model = get_seq_model(VGGnet).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction of VGG16\n",
    "We now perform the reduction of VGG16 using the module ***NetAdapter***. In this case we use 5 as cut off index and 50 as dimension of the reduced space. For the reduced method and the input-output mapping there are multiple choices: 'POD', 'AS', 'RandSVD' or 'HOSVD' for the first one, and 'PCE' or 'FNN' for the latter. In the following cells we are going to provide some examples of possible combinations of the aforementioned techniques. The Figure below summarizes the reduction method proposed, as described in the article ''A Dimensionality Reduction Approach for Convolutional Neural Networks'', Meneghetti L., Demo N., Rozza G., https://arxiv.org/abs/2110.09163 (2021).\n",
    "\n",
    "<img src = \"red_cnn.png\" style = \"height:400px\">\n",
    "\n",
    "Let's start by computing the current accuracy of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the full network on test images is 87.5100\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "seq_model.eval()\n",
    "for test, y_test in iter(test_loader):\n",
    "#Calculate the class probabilities (softmax) for img\n",
    "    with torch.no_grad():\n",
    "        output = seq_model(test.to(device)).to(device)\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test.to(device)).sum().item()\n",
    "    \n",
    "print(\"Accuracy of the full network on test images is {:.4f}\".format(100*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POD + FNN\n",
    "The first method we describe uses POD as reduction technique and a Feedforward Neural Network (FNN) as input-output mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cutoff_idx = 7 \n",
    "red_dim = 50 \n",
    "red_method = 'POD' \n",
    "inout_method = 'FNN'\n",
    "n_class = 10\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, train_labels, train_loader, n_class)\n",
    "print(red_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandSVD + FNN\n",
    "A small variant of the previous case can be obtained using Random SVD as reduction technique and a Feedforward Neural Network (FNN) as input-output mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_idx = 5 \n",
    "red_dim = 50 \n",
    "red_method = 'RandSVD' \n",
    "inout_method = 'FNN'\n",
    "n_class = 10\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, train_labels, train_loader, n_class)\n",
    "print(red_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHOSVD + FNN\n",
    "A different choice is represented by the introduction of HOSVD as reduction technique that keeps into account the tensorial structure of the objects under consideration. Hence, in this case we are using a variant of HOSVD, called Averaged HOSVD (AHOSVD), which performs HOSVD in batches and then computes the average between them to overcome the high computational effort needed. In particular, we are the n coupling AHOSVD with FNN as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN training initialized\n",
      "FNN training completed\n",
      "RedNet(\n",
      "  (premodel): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (proj_model): tensor_product_layer(in_dimensions=[256, 4, 4], out_dimensions=[35, 3, 3])\n",
      "  (inout_map): FNN(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=315, out_features=20, bias=True)\n",
      "      (1): Softplus(beta=1, threshold=20)\n",
      "      (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cutoff_idx = 7\n",
    "red_method= 'HOSVD'\n",
    "red_dim = [35, 3, 3]\n",
    "inout_method = 'FNN'\n",
    "n_class = 10  \n",
    "\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, train_labels, train_loader, n_class, device = device).to(device) \n",
    "print(red_model, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the reduced network\n",
    "Now that the reduced network has been defined, we can train it. The technique used is \"knowledge distillation\", i.e. try to use the knowledge contained in the original full model, also referred as the teacher model, to train the the reduced model, called student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 0.4235516825962067\n",
      " Top 1:  Accuracy: 4614.0/50000 (9.23%)\n",
      "Loss Value: 8.471033651924133e-06\n",
      "Test Loss 0.5047410353899002\n",
      " Top 1:  Accuracy: 841.0/10000 (8.41%)\n",
      "Loss Value: 5.047410353899002e-05\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lmeneghe/anaconda/lib/python3.8/site-packages/torch/nn/functional.py:2747: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss kd: 1.5311928987503053e-05\n",
      "Test Loss -0.029346163740754126\n",
      " Top 1:  Accuracy: 8097.0/10000 (80.97%)\n",
      "Loss Value: -2.9346163740754128e-06\n",
      "EPOCH 2\n",
      "Train Loss kd: 3.4757274389266966e-06\n",
      "Test Loss 0.4364694202244282\n",
      " Top 1:  Accuracy: 8363.0/10000 (83.63%)\n",
      "Loss Value: 4.364694202244282e-05\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "optimizer = torch.optim.Adam([{\n",
    "            'params': red_model.premodel.parameters(),\n",
    "            'lr': 1e-4\n",
    "            }, {\n",
    "            'params': red_model.proj_model.parameters(),\n",
    "            'lr': 1e-5\n",
    "            }, {\n",
    "            'params': red_model.inout_map.parameters(),\n",
    "            'lr': 1e-5\n",
    "            }])\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_loss.append(compute_loss(red_model, device, train_loader))\n",
    "test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "\n",
    "        \n",
    "epochs = 2\n",
    "filename = './cifar10_VGG16_RedNet'+red_method+'_cutIDx_%d.pth'%(cutoff_idx)\n",
    "for epoch in range(1, epochs + 1):                     \n",
    "    print('EPOCH {}'.format(epoch), flush=True)\n",
    "    train_loss.append(\n",
    "            train_kd(red_model,\n",
    "            VGGnet,\n",
    "            device,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            train_max_batch=200,\n",
    "            alpha=0.1,\n",
    "            temperature=1.,\n",
    "            epoch=epoch))\n",
    "    test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "torch.save(copy.deepcopy(red_model), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a reduced network checkpoint\n",
    "If a reduced network has been already defined and saved on the computer, it can be loaded with the following instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = filename #path to the checkpoint file\n",
    "red_model = torch.load(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy testing\n",
    "We can further test the accuracy of the network with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the full network on test images is 83.6300\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for test, y_test in iter(test_loader):\n",
    "    with torch.no_grad():\n",
    "        output = red_model(test.to(device))\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test.to(device)).to(device).sum().item()\n",
    "\n",
    "print(\"Accuracy of the full network on test images is {:.4f}\".format(100*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage and flops needed for the model\n",
    "The following lines of code provide the amounts of storage needed to save the reduced model together with the number of floating point operations needed to compute them.\n",
    "\n",
    "We start by counting the number of non-zero entries (nnz) of the three components of the reduced network (this method only concerns the POD+FNN and RandSVD+FNN techniques, regarding the storage) and the flops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre nnz = 6.62, proj_model nnz=0.03, FNN nnz=0.0249\n",
      "flops:  Pre = 190.51, proj_model = 0.00, FNN =0.01\n"
     ]
    }
   ],
   "source": [
    "rednet_storage = torch.zeros(3)\n",
    "rednet_flops = torch.zeros(3)\n",
    "\n",
    "rednet_storage[0], rednet_storage[1], rednet_storage[2] = [\n",
    "    Total_param(red_model.premodel),\n",
    "    Total_param(red_model.proj_model),\n",
    "    Total_param(red_model.inout_map)]\n",
    "\n",
    "rednet_flops[0], rednet_flops[1], rednet_flops[2] = [\n",
    "    Total_flops(red_model.premodel, device),\n",
    "    Total_flops(red_model.proj_model, device),\n",
    "    Total_flops(red_model.inout_map, device)]\n",
    "\n",
    "\n",
    "print('Pre nnz = {:.2f}, proj_model nnz={:.2f}, FNN nnz={:.4f}'.format(\n",
    "      rednet_storage[0], rednet_storage[1],\n",
    "      rednet_storage[2]))\n",
    "print('flops:  Pre = {:.2f}, proj_model = {:.2f}, FNN ={:.2f}'.format(\n",
    "       rednet_flops[0], rednet_flops[1], rednet_flops[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define another method that counts the storage needed for saving the reduced model (in MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the storage needed by the RedNet model.\n",
      "Components summary:\n",
      "premodel.0.weight \t torch.Size([64, 3, 3, 3])\n",
      "premodel.0.bias \t torch.Size([64])\n",
      "premodel.2.weight \t torch.Size([64, 64, 3, 3])\n",
      "premodel.2.bias \t torch.Size([64])\n",
      "premodel.5.weight \t torch.Size([128, 64, 3, 3])\n",
      "premodel.5.bias \t torch.Size([128])\n",
      "premodel.7.weight \t torch.Size([128, 128, 3, 3])\n",
      "premodel.7.bias \t torch.Size([128])\n",
      "premodel.10.weight \t torch.Size([256, 128, 3, 3])\n",
      "premodel.10.bias \t torch.Size([256])\n",
      "premodel.12.weight \t torch.Size([256, 256, 3, 3])\n",
      "premodel.12.bias \t torch.Size([256])\n",
      "premodel.14.weight \t torch.Size([256, 256, 3, 3])\n",
      "premodel.14.bias \t torch.Size([256])\n",
      "proj_model.param0 \t torch.Size([35, 256])\n",
      "proj_model.param1 \t torch.Size([3, 4])\n",
      "proj_model.param2 \t torch.Size([3, 4])\n",
      "inout_map.model.0.weight \t torch.Size([20, 315])\n",
      "inout_map.model.0.bias \t torch.Size([20])\n",
      "inout_map.model.2.weight \t torch.Size([10, 20])\n",
      "inout_map.model.2.bias \t torch.Size([10])\n",
      "\n",
      "\n",
      "The MB used are: 7.004007816314697\n"
     ]
    }
   ],
   "source": [
    "print('Computing the storage needed by the RedNet model.\\nComponents summary:')\n",
    "storage = 0\n",
    "for param_tensor in red_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", red_model.state_dict()[param_tensor].size())\n",
    "    storage += torch.prod(torch.tensor(list(red_model.state_dict()[param_tensor].size())))\n",
    "print(f\"\\n\\nThe MB used are: {4 * storage / 10 ** 6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c5bf16c94eb6f9341fa612a12f652937166e39821fa969ec7095b77ab48ffd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
